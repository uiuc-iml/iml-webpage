<h2>Data-efficient robot learning for decision making in the real world using knowledge of physics and prior experience</h2>

<h3>Yifan Zhu and Kris Hauser</h3>

<img src="data_collection.gif" width="195" height="150" /> 
<img src="lamb1.gif" width="125" height="150" /> 
<img src="lamb2.gif" width="125" height="150" /> 
<img src="Auscultation.gif" width="200" height="150" /> 
<img src="scooping.gif" width="180" height="150" /> 
<img src="TO.gif" width="500" height="150" /> 

<h4>Summary</h4>

<p>Recent breakthroughs have made deep learning a popular approach in various fields of engineering and science, including robotics, but it is data-hungry and transfers poorly to new tasks once trained. Hence, compared to other fields like vision and NLP, learning has not yet made as much of an impact in robotics where data is often scarce and expensive to collect. This project investigates data-efficient learning methods that 1) taking advantage of knowledge of physics and performing "gray-box" learning and 2) using prior experience on similar tasks.</p>

<p>We have applied our approach on 3 robotics tasks in which gray-box learning has substantially improved data-efficiency compared to black-box learning. Our approaches are applied to locomotion granular media, visuo-tactile model learning of deformable objects, and automated heart and lung auscultation.  Beyond the problem of learning models, we also address decision-making problems that use such models for dynamics, reward, and/or constraint prediction.  </p>

<p> Highlights of this work include the introduction of a novel bilevel trajectory optimization techniques for legged robots to traverse sandy terrain, where the sand-foot interaction behavior is learned with a few hundred touches.  We also demonstrate a novel deep Gaussian Process meta-learning technique based on maximizing the deployment gap during meta-training, which enables a robot to use vision and just a few online manipulation attempts to achieve high-quality scooping actions on out-of-distribution granular terrains.  This model was transferred in zero-shot fashion to the JPL OWLAT testbed, and outperformed all other models tested.</p>


    <table style="max-width:1200px;border:0px;border-spacing:0px;border-collapse:separate;"><tbody>
    <!-- <table style="width:100%;max-width:1200px;border:0px;border-spacing:0px;border-collapse:separate;"><tbody> -->
          <tr style="padding:0px">
            <td style="padding:0px">  
                <td style="padding:0px;vertical-align:middle">
                    <!-- <td style="padding:0px;width:40%;vertical-align:left"> -->
                <div class="one">
                    <img src="CoDeGa.png" width=400px />
                </div>
                </td>
                <td style="min-width:300px;padding:20px;vertical-align:left">
                <h4>Meta-learning few-shot scooping</h4>
                <p>Autonomous lander missions on extraterrestrial bodies will need to sample granular material while coping with domain shift, no matter how well a sampling strategy is tuned on Earth. This paper proposes an adaptive scooping strategy that uses deep Gaussian process method trained with meta-learning to learn on-line from very limited experience on the target terrains. It introduces a novel meta-training approach, Deep Meta-Learning with Controlled Deployment Gaps (CoDeGa), that explicitly trains the deep kernel to predict scooping volume robustly under large domain shifts. Employed in a Bayesian Optimization sequential decision-making framework, the proposed method allows the robot to use vision and very little on-line experience to achieve high-quality scooping actions on out-of-distribution terrains, significantly outperforming non-adaptive methods proposed in the excavation literature as well as other state-of-the-art meta-learning methods. Moreover, a dataset of 6,700 executed scoops collected on a diverse set of materials, terrain topography, and compositions is made available for future research in granular material manipulation and meta-learning.</p>
                
                <a href="https://arxiv.org/abs/2303.02893"><img class="link"  src="../images/link_icon_tiny.gif" alt="link"></a>
                <a href="https://drillaway.github.io/scooping-dataset.html"><img class="sw"  src="../images/sw_icon_tiny.gif" alt="software"></a>
            </td>
            </td>
          </tr>
    
          <tr style="padding:0px">
            <td style="padding:0px">  
                <!-- <td style="padding:90px;width:40%;vertical-align:left"> -->
                <td style="padding:0px;vertical-align:middle">
                <div class="one">
                    <img src="Auscultation_full.gif" width=300px />
                </div>
                </td>
                <td style="min-width:300px;padding:20px;vertical-align:left">
                <h4>Automated Heart and Lung Auscultation in Robotic Physical Examinations</h4>
                <p> This paper presents the first implementation of autonomous robotic auscultation of heart and lung sounds. To
                    select auscultation locations that generate high-quality sounds,
                    a Bayesian Optimization (BO) formulation leverages visual
                    anatomical cues to predict where high-quality sounds might be
                    located, while using auditory feedback to adapt to patient-specific
                    anatomical qualities. Sound quality is estimated online using
                    machine learning models trained on a database of heart and lung
                    stethoscope recordings. Experiments on 4 human subjects show
                    that our system autonomously captures heart and lung sounds of
                    similar quality compared to tele-operation by a human trained in
                    clinical auscultation. Surprisingly, one of the subjects exhibited
                    a previously unknown cardiac pathology that was first identified
                    using our robot, which demonstrates the potential utility of
                    autonomous robotic auscultation for health screening.</p> <a href="../papers/RAL2022-Zhu-Ascultation-preprint.pdf"><img class="pdf"  src="../images/pdf_icon_tiny.gif" alt="pdf"></a>
                    <a href="https://arxiv.org/abs/2201.09511"><img class="link"  src="../images/link_icon_tiny.gif" alt="link"></a>
                </td>
            </td>
          </tr>
   
        <tr style="padding:0px">
            <td style="padding:0px">  
                <td style="padding:0px;vertical-align:left">
                <div class="one">
                    <img src="poke.gif" width=400px/>
                </div>
                </td>
                <td style="min-width:300px;padding:20px;vertical-align:left">
                <h4>Semi-Empirical Simulation of Learned Force Response Models for Heterogeneous Elastic Objects</Object></h4>
                <p>This paper presents a semi-empirical method for
                    simulating contact with elastically deformable objects whose
                    force response is learned using entirely data-driven models.
                    A point-based surface representation and an inhomogeneous,
                    nonlinear force response model are learned from a robotic arm
                    acquiring force-displacement curves from a small number of
                    poking interactions. The simulator then estimates displacement
                    and force response when the deformable object is in contact
                    with an arbitrary rigid object. It does so by estimating displacements by solving a Hertzian contact model, and sums the
                    expected forces at individual surface points through querying
                    the learned point stiffness models as a function of their expected
                    displacements. Experiments on a variety of challenging objects
                    show that our approach learns force response with sufficient
                    accuracy to generate plausible contact response for novel rigid
                    objects.</p>
                    <a href="../papers/ICRA2020-Zhu-LearningElastic.pdf"><img class="pdf"  src="../images/pdf_icon_tiny.gif" alt="pdf"></a>
                    <a href="https://youtu.be/-9sjiERznRc"><img class="mov" src="../images/mov_icon_tiny.gif"> Summary video</a>
                    <a href="https://youtu.be/xdrKUddJaEU"><img class="mov" src="../images/mov_icon_tiny.gif"> ICRA 10-minute talk video</a>
                </td>
            </td>
          </tr>


        <tr style="padding:0px">
            <td style="padding:0px">  
                <td style="padding:0px;vertical-align:left">
                <div class="one">
                    <img src="TO_both.gif" width=400px />
                </div>
                </td>
                <td style="min-width:300px;padding:20px;vertical-align:left">
                <h4>Contact-Implicit Trajectory Optimization with Learned Deformable Contacts Using Bilevel Optimization</h4>
                <p> Based on the previous project shown below, we present a bilevel, contact-implicit trajectory
                    optimization (TO) formulation that searches for robot trajectories with learned soft contact models. On the lower-level, contact
                    forces are solved via a quadratic program (QP) with the maximum dissipation principle (MDP), based on which the dynamics
                    constraints are formulated in the upper-level TO problem that
                    uses direct transcription. Our method uses a contact model
                    for granular media that is learned from physical experiments,
                    but is general to any contact model that is stick-slip, convex,
                    and smooth. We employ a primal interior-point method with
                    a pre-specified duality gap to solve the lower-level problem,
                    which provides robust gradient information to the upper-level
                    problem. We evaluate our method by optimizing locomotion
                    trajectories of a quadruped robot on various granular terrains
                    offline, and show that we can obtain long-horizon walking gaits
                    of high qualities.</p>
                    <a href="papers/ICRA2021_Zhu_TrajectoryOptimizationDeformable.pdf"><img class="pdf"  src="../images/pdf_icon_tiny.gif" alt="pdf"></a>
                    <a href="https://youtu.be/cgMl3bAGfqc"> <img class="mov" src="../images/mov_icon_tiny.gif"> Supplemental video</a>
                </td>
            </td>
          </tr>



      <tr style="padding:0px">
        <td style="padding:0px">  
            <td style="padding:0px;vertical-align:left">
            <div class="one">
               <img src="GM.gif" width=400px/>
            </div>
            </td>
            <td style="min-width:300px;padding:20px;vertical-align:left">
            <h4>A Data-driven Approach for Fast Simulation of Robot Locomotion on Granular Media</h4>
            <p>In this paper, we propose a semi-empirical approach for simulating robot locomotion on granular media. We
                first develop a contact model based on the stick-slip behavior
                between rigid objects and granular grains, which is then learned
                through running extensive experiments. The contact model
                represents all possible contact wrenches that the granular
                substrate can provide as a convex volume, which our method
                formulates as constraints in an optimization-based contact force
                solver. During simulation, granular substrates are treated as
                rigid objects that allow penetration and the contact solver solves
                for wrenches that maximize frictional dissipation. We show that
                our method is able to simulate plausible interaction response
                with several granular media at interactive rates.</p>
                <a href="../papers/ICRA2019-Zhu-GranularMedia.pdf"><img class="pdf"  src="../images/pdf_icon_tiny.gif" alt="pdf"></a> <a href="https://youtu.be/AVyvnFREed8"><img class="mov" src="../images/mov_icon_tiny.gif"> ICRA video</a>
                    

            <!-- <a href="https://waymo.com/research/block-nerf/">
                <papertitle>NeRF-Supervision: Learning Dense Object Descriptors from Neural Radiance Fields</papertitle>
            </a>
            <br>
            <a href="https://yenchenlin.me/">Lin Yen-Chen</a>, 
            <a href="http://www.peteflorence.com/">Pete Florence</a>, 
            <strong>Jonathan T. Barron</strong>,  <br>
            <a href="https://scholar.google.com/citations?user=_BPdgV0AAAAJ&hl=en">Tsung-Yi Lin</a>, 
            <a href="https://meche.mit.edu/people/faculty/ALBERTOR@MIT.EDU">Alberto Rodriguez</a>,
            <a href="http://web.mit.edu/phillipi/">Phillip Isola</a>
            <br>
            <em>ICRA</em>, 2022  
            <br>
                            <a href="http://yenchenlin.me/nerf-supervision/">project page</a> / 
                            <a href="https://arxiv.org/abs/2203.01913">arXiv</a> / 
                            <a href="https://www.youtube.com/watch?v=_zN-wVwPH1s">video</a> /
                            <a href="https://github.com/yenchenlin/nerf-supervision-public">code</a> / 
                            <a href="https://colab.research.google.com/drive/13ISri5KD2XeEtsFs25hmZtKhxoDywB5y?usp=sharing">colab</a>				
            <p></p> -->
            </td>
        </td>
      </tr>
    </table>


<div class="pubs">
<ul class="publications">
	<li>P. Thangeda, A. Goel, E. Tevere, Y. Zhu, E. Kramer, A. Daca, H. Nayar, K. Hauser, and M. Ornik. <i>Learning and Autonomy for Extraterrestrial Terrain Sampling: An Experience Report from OWLAT Deployment</i>. 2024 AIAA SciTech, January 8, 2024.
	<span class="nw">
		<a href="../papers/AIAAScitech2024_CoDeGa_Deployment.pdf"><img class="pdf"  src="../images/pdf_icon_tiny.gif" alt="pdf"></a>
	</span></li>
	<li>Y. Zhu. <i>Data-Efficient Learning for Manipulation, Locomotion, and Information Gathering Involving Granular Media and Deformable Objects</i>. Ph.D. Thesis, University of Illinois, Urbana-Champaign, 2023.
	<span class="nw">
		<a href="../papers/PhDThesis_2023_Zhu_DataEfficient.pdf"><img class="pdf"  src="../images/pdf_icon_tiny.gif" alt="pdf"></a>
	</span></li>
    <li>Y. Zhu*, Pranay Thangeda*, M. Ornik and K. Hauser. <i>Few-shot Adaptation for Manipulating Granular Materials Under Domain Shift</i>. Robotics: Science and Systems (RSS) 2023.
        <span class="nw">
            <a href="https://arxiv.org/abs/2303.02893"><img class="link"  src="../images/link_icon_tiny.gif" alt="link"></a>
            <a href="https://drillaway.github.io/scooping-dataset.html"><img class="sw"  src="../images/sw_icon_tiny.gif" alt="software"></a>
    </span></li>
	<li>Y. Zhu, A. Smith, and K. Hauser. <i>Automated Heart and Lung Auscultation in Robotic Physical Examinations</i>. IEEE Robotics and Automation Letters, 2022.
        <span class="nw">
            <a href="../papers/RAL2022-Zhu-Ascultation-preprint.pdf"><img class="pdf"  src="../images/pdf_icon_tiny.gif" alt="pdf"></a>
            <a href="https://arxiv.org/abs/2201.09511"><img class="link"  src="../images/link_icon_tiny.gif" alt="link"></a>
    </span></li>
    <li>Y. Zhu, K. Lu, and K. Hauser. <i>Semi-Empirical Simulation of Learned Force Response Models for Heterogeneous Elastic Objects</i>. IEEE International Conference on Robotics and Automation (ICRA), June 2020.
	<span class="nw"><a href="../papers/ICRA2020-Zhu-LearningElastic.pdf"><img class="pdf"  src="../images/pdf_icon_tiny.gif" alt="pdf"></a> <a href="https://youtu.be/-9sjiERznRc"><img class="mov" src="../images/mov_icon_tiny.gif"> Summary video</a> <a href="https://youtu.be/xdrKUddJaEU"><img class="mov" src="../images/mov_icon_tiny.gif"> ICRA 10-minute talk video</a></span>
	</li>
    <li>Y. Zhu, Z. Pan, and K. Hauser. <i>Contact-Implicit Trajectory Optimization with Learned Deformable Contacts Using Bilevel Optimization</i>. IEEE International Conference on Robotics and Automation (ICRA), May 2021. <span class="nw">
        <a href="papers/ICRA2021_Zhu_TrajectoryOptimizationDeformable.pdf"><img class="pdf"  src="../images/pdf_icon_tiny.gif" alt="pdf"></a>
        <a href="https://youtu.be/cgMl3bAGfqc"> <img class="mov" src="../images/mov_icon_tiny.gif"> Supplemental video</a>
    </span></li>
    <li>Y. Zhu, L. Abdulmajeid, and K. Hauser. <i>Data-driven Approach for Fast Simulation of Robot Locomotion on Granular Media</i>. IEEE International Conference on Robotics and Automation, May, 2019. <span class="nw"><a href="../papers/ICRA2019-Zhu-GranularMedia.pdf"><img class="pdf"  src="../images/pdf_icon_tiny.gif" alt="pdf"></a> <a href="https://youtu.be/AVyvnFREed8"><img class="mov" src="../images/mov_icon_tiny.gif"> ICRA video</a> </span></li>
</ul>
</div>


<div class="sponsors">
    <div class="sponsor">
        <img src="../images/logos/nsf1.gif">
        NSF NRI Grant #1527826<br>
        NSF NRI Grant #1911087<br>
        NSF NRI Grant #2025782<br>
        NSF NRI Grant #1830366
    </div>
    <div class="sponsor">
        <img src="../images/logos/NASA.jpg" width="120" height="100" >
    </div>
</div>
